%http://www.cmap.polytechnique.fr/~allaire/map411/MiniProjets/sujets2016/sujet_Augier.pdf

% --------------------------------------------------------------
%                         Template
% --------------------------------------------------------------

\documentclass[12pt]{article} %draft = show box warnings
\usepackage[utf8]{inputenc} % Accept different input encodings [utf8]
\usepackage[T1]{fontenc}    % Standard package for selecting font encodings
\usepackage{lmodern} % Good looking T1 font
\usepackage[a4paper, total={6.5in,10.2in}]{geometry} % Flexible and complete interface to document dimensions
\renewcommand{\baselinestretch}{1.0}

% --------------------------------------------------------------
%                         Other Fonts
% --------------------------------------------------------------
%\usepackage{mathpazo} % Hermann Zapf's Palatino font
%\usepackage{kpfonts} % Kepler font
%\usepackage{mathptmx} % Times New Roman Like Font
%\usepackage{eulervm} %  AMS Euler (eulervm) math font.

% --------------------------------------------------------------
%                         Package
% --------------------------------------------------------------
\usepackage[french]{babel} % Multilingual support [french]
\usepackage{enumerate} % Enumerate with redefinable labels
\usepackage{graphicx} % Enhanced support for graphics
\usepackage[makeroom]{cancel} % Place lines through maths formulae
\usepackage{booktabs} % Publication quality tables
\usepackage{braket} % Dirac bra-ket and set notations
\usepackage{epstopdf} % Convert EPS to ‘encapsulated’ PDF using Ghostscript
\usepackage{bbm} % "Blackboard-style" cm fonts
\RequirePackage{epsfig} % Include Encapsulated PostScript
\usepackage{float} % Improved interface for floating objects
\usepackage{amsmath,amsthm,amssymb} % American Mathematics Society facilities

\usepackage{stmaryrd} % St Mary Road symbols for theoretical computer science.
\usepackage{xfrac} % Create inline fraction
\usepackage{hyperref} % Create hyperf
\usepackage{secdot} % Put dots at the end of a section
\usepackage{comment}



%------------------- tikz --------------------------------------
\usepackage{tikz,bm,color}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\usetikzlibrary{shapes.geometric,arrows,positioning}
\usetikzlibrary{calc}

% --------------------------------------------------------------
%                         Custom commands
% --------------------------------------------------------------
\newcommand*{\1}{\mathbbm{1}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\Pb}{\mathbb{P}}
\newcommand*{\N}{\mathbb{N}} 
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\car}{\text{card}}
\renewcommand*{\Re}{\operatorname{Re}}
\renewcommand*{\Im}{\operatorname{Im}}
\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}
\newcommand*{\grad}{\nabla}
\renewcommand*{\O}{\mathcal{O}}

% --------------------------------------------------------------
%                         Exercise Environment
% --------------------------------------------------------------
\renewcommand{\thesection}{\Roman{section}}
\sectiondot{subsection}
\renewcommand{\subsubsection}{
	\pagebreak[2]
	\refstepcounter{subsubsection}
	\noindent
	\textbf{\thesubsubsection.}
}
% --------------------------------------------------------------

\begin{document}
	% --------------------------------------------------------------
	%                       Title  Header
	% --------------------------------------------------------------
	\noindent
	\normalsize\textbf{Approximation numérique et optimisation} \hfill \textbf{École Polytechnique}\\
	\normalsize\textbf{MAP 411} \hfill \textbf{\today}\vspace{20pt}
	\centerline{\Large Projet MAP411 – X2015}\vspace{5pt}
	\centerline{\Large \textbf{Applications de méthodes d'optimisation à la résolution des EDP}}\vspace{3pt}
	\centerline{Sujet proposé par Nicolas Augier -- \texttt{nicolas.augier@ens-cachan.fr}  }\vspace{13pt}
	\centerline{Lucas Lugão Guimarães -- \texttt{lucas.lugao-guimaraes@polytechnique.edu}  }
	\centerline{Alexandre Ribeiro João Macedo --  \texttt{alexandre.macedo@polytechnique.edu}}\vspace{20pt}
	% --------------------------------------------------------------
	
	\section{Méthodes de gradient}
	\subsection{Gradient à pas constant}
	\subsubsection{} %%I.1.1
	La figure \ref{fig:I11} montre les lignes de niveau de la fonction de Rosenbrock,
	\begin{align} \label{eq:rosenbrock}
	J(u_1,u_2)=(u_1-1)^2+100(u_1^2-u_2)^2 \text{,}
	\end{align}
	dans le rectangle $(u_1, u_2) \in [-1.5, 1.5] \textmd{ x } [0.5, 1.5]$.   
	\begin{figure}[ht]
		\begin{center}
    \hspace*{-1cm}  
	\includegraphics[height=8cm]{I11.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Lignes de niveau de $J$.}
		\label{fig:I11}
	\end{figure}
	
    \begin{figure}[ht]
		\begin{center}
    \hspace*{-3cm}  
	\includegraphics[height=8cm]{I13d.png}
        \vspace{-1cm}
		\end{center}
		\caption{Surface plot de la fonction de Rosenbrock.}
		\label{fig:I13d}
	\end{figure}
	\subsubsection{} %% I.1.2
	On applique la méthode de gradient a pas constant $u_{k+1}=u_k-\rho \grad J(u_k)$ ou, pour $ u_k= (u_{k1},u_{k2})$, on a $\grad J(u_k) = ({400u_{k1}^3 + 2u_{k1}  - 400u_{k1}u_{k2} - 2}, {200u_{k1}^2 - 200u_{k2}})$. Pour les valeurs numériques $u_0=(1, 0)$ et $\rho = 0.002$, et le critère d'arrêt $J(u_{k+1}) < 10^{-3}$, on a besoin de \textbf{$2764$} itérations. La figure \ref{fig:I12p} montre les lignes de niveau de l'équation \ref{eq:rosenbrock} et le parcours du méthode avant l'arrête.
	\begin{figure}[H]
		\begin{center}
	\includegraphics[height=8cm]{I12002.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Parcours du méthode de gradient à pas constant.}
		\label{fig:I12p}
	\end{figure}
	
	\subsubsection{} %% I.1.3
	Si on applique la méthode pour $\rho = 0.0045$ et pour $\rho = 0.01$, on a que le méthode diverge. Pour la valeur $\rho = 0.0045$, la méthode oscille en tour du minimum et ne converge pas. Pour la valeur  $\rho = 0.01$, la première itération est déjà très loin du point de minimum et il ne reviens pas.
	\begin{figure}[H]
		\begin{center}
    \hspace*{-1.5cm}  
	\includegraphics[height=6cm]{I12d.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Le parcours du méthode de gradient à pas constant $\rho = 0.045$ et $\rho = 0.01$.}
		\label{fig:I12d}
    \end{figure}
Toutes les figures et résultats ci-dessus peuvent être crée par le code "problem1.py".
    
    \subsection{Gradient optimal}
\subsubsection{} %% II.1.1
	Soit $J(u)=\frac{1}{2} \langle Au, u \rangle - \langle b, u \rangle$. On veut trouver ${\rho_k=\textrm{argmin}_{\rho \geq 0} J(u_k- \rho \grad J(u_k))}$. Soit ${r_k= \grad J(u_k) = Au_k-b}$, on a
	\begin{align*}
	J(u_k - \rho \grad J(u_k)) &= J(u_k - \rho r_k) \\ 
	&=  \frac{1}{2} \langle A(u_k - \rho r_k), u_k - \rho r_k \rangle - \langle b, u_k - \rho r_k \rangle \\
	&=\frac{1}{2} \langle Au_k, u_k \rangle - \langle b, u_k \rangle  + \frac{1}{2} \langle Au_k, - \rho r_k \rangle + \frac{1}{2} \langle - \rho A r_k, u_k \rangle + \frac{1}{2} \langle - \rho A r_k,  - \rho r_k \rangle  \\
	& \hphantom{==\frac{1}{2} \langle Au_k, u_k \rangle - \langle b, u_k \rangle  + \frac{1}{2} \langle Au_k, - \rho r_k \rangle + \frac{1}{2} \langle - \rho A r_k, u_k \rangle } - \langle b, - \rho r_k \rangle \\
    &= J(u_k) - \frac{\rho}{2} [\langle A u_k, r_k \rangle + \langle A r_k, u_k \rangle]+\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho \langle b,  r_k \rangle \\
    &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho [\langle b,  r_k \rangle -\langle A u_k, r_k \rangle] \\
    &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho [\langle b,  r_k \rangle -\langle r_k - b, r_k \rangle] \\
      &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho \langle r_k, r_k \rangle \textrm{.}
	\end{align*}
    
Alors, la valeur que minimise cette fonction quadratique en $\rho$ est 
	\begin{align*}
	\rho_k=\frac{\langle r_k, r_k \rangle}{\langle A r_k, r_k \rangle}
	\end{align*}
\subsubsection
Pour les valeur $A=\begin{pmatrix} 1 & 0 \\  0 & \lambda \end{pmatrix}$ et  $b= \begin{pmatrix} 1 \\1 \end{pmatrix}$ on a
\begin{align*}
    	\begin{pmatrix}
    	r_{k1} \\
    	r_{k2}
    	\end{pmatrix}
    =    
 \begin{pmatrix}
1 & 0 \\
0 & \lambda
\end{pmatrix} 
\begin{pmatrix}
u_{k1} \\
u_{k2}
\end{pmatrix}
-
    \begin{pmatrix}
1 \\
1
\end{pmatrix}
=
 \begin{pmatrix}
u_{k1} - 1 \\
\lambda u_{k2} -1
\end{pmatrix}
\text{. Alors,}
\end{align*}
\begin{align} \label{eq:rho_k}
\rho_k = \frac{(u_{k1} - 1)^2 + (\lambda u_{k2} - 1)^2}{(u_{k1} - 1)^2 + \lambda (\lambda u_{k2} - 1)^2 }
\end{align}

À chaque itération, on actualise $\rho_k$ en utilisant l'équation \ref{eq:rho_k}. Le figure \ref{fig:I23a} montre les résultats pour différentes condition initiales et pour le critère de arrête 
\begin{align*}
\frac{\| r_k \|}{\| r_0 \|} \leq 10^{-2}\text{.}
\end{align*}
\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I123a.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours du méthode de gradient à pas optimal.}
	\label{fig:I23a}
\end{figure}
\subsubsection
Les figure \ref{fig:I23a} et \ref{fig:I23b} montre en échelle semi logarithme le rapport $\frac{\| r_k \|}{\| r_0 \|}$ en fonction de $k$ pour les cas montrés dans les figure \ref{fig:I23a} et \ref{fig:I23b}. Ces résultats peuvent être trouver en utilisant le code python disponible dans "problem2.py"

\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I123b.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours du méthode de gradient à pas optimal.}
	\label{fig:I23b}
\end{figure}
	
\subsubsection
Quand on met en œuvre le procédé de dichotomie pour l'équation \ref{eq:rosenbrock}, on trouve le résultat ci-dessous. Ce résultat a été géré par le code disponible dans le fichier "problem3.py". 
\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I124.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours obtenu à partir du procédé de dichotomie pour trouver le minimum global de la fonction de Rosenbrock.}
	\label{fig:I24}
\end{figure}
	
\section{Méthodes newtoniennes}
\subsection{Méthode de Newton}
\subsection{Méthode de Gauss-Newton}
\subsubsection{}
On note le vecteur des paramètres $\beta = \{\beta_1, \dots, \beta_{3K}\}$ de manière que $\beta_j = \alpha_j$, $\beta_{j+K} = \sigma_{j}$ et $\beta_{j+2K} = x^0_j$. Ainsi la matrice jacobienne $Df(\beta)$ de dimension $N\times3K$ est donné par $\left[Df(\beta)\right]_{ij} = \frac{\partial f_i(\beta)}{\partial \beta_j}$. À fin de la calculer on rappelle la définition du vecteur $f$ $f_i(\beta) = y_i - g(x_i, \beta)$ avec $g(x_i,\beta) = \sum_{j = 1}^K \alpha_j \exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right)$
\begin{align*}
\left[Df(\beta)\right]_{ij} = \frac{\partial f_i(\beta)}{\partial \alpha_j} &= - \exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K\\
\left[Df(\beta)\right]_{i(K+j)} = \frac{\partial f_i(\beta)}{\partial \sigma_j} &= -\alpha_j \frac{(x-x_j^0)^2}{\sigma_j^3}\exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K\\
\left[Df(\beta)\right]_{i(2K+j)} =\frac{\partial f_i(\beta)}{\partial x_j^0} &= -\alpha_j \frac{(x-x_j^0)}{\sigma_j^2}\exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K
\end{align*}

On écrit le processus itératif à partir de l'algorithme de Newton $\beta_{k+1} = \beta_{k} - [DF(\beta_k)]^{-1}F(\beta_k)$ en prenant l'approximation $DF(\beta_k) = D [\nabla J(\beta_k)] = \nabla^2 J(\beta_k) \approx Df^T(u)Df(u)$ et en rappelant que $F(\beta_k) = \nabla J(\beta_k) =  Df^T(\beta_{k})f(\beta_{k})$
\begin{align*}
\beta_{k+1} = \beta_{k} - [Df^T(\beta_{k})Df(\beta_{k})]^{-1} Df^T(\beta_{k})f(\beta_{k}),\quad k\geq 0
\end{align*}
\subsubsection{}
Avec les conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 7$ la méthode de Gauss-Newton converge vers les valeurs suivantes
\begin{align*}
\alpha_1 = 0.22213775,\quad
\alpha_2 = 0.98932187,\quad
\sigma_1 = 1.27128674,\\
\sigma_2 = 2.64012043,\quad
x^0_1 = 2.57935799,\quad
x^0_2 = 9.16230523          
\end{align*}
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{I22.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Nuage de points obtenue à partir des conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 7$}
	\label{fig:I22}
\end{figure}
\subsubsection{}
D'après la figure \ref{fig:I23} on voit la divergence de la méthode de Gauss-Newton avec des conditions initiales proches de la solution trouvé dans l'item précédent. De cette manière on voit la non-convergence locale de cette approximation de la Méthode de Newton classique.
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{I23.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Nuage de points obtenue à partir des conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 6$}
	\label{fig:I23}
\end{figure}

Les résultats ci-dessus ont été crée par le code disponible dans "problem4.py".
\section{Application à l'équation de convection-diffusion}
\subsection{Différences schémas d'approximation}
\subsubsection{}
On remplace $u_i^n$ par $u(x_i, t_n)$ où $u$ est une fonction régulière. On commence par le développement de Taylor au tour du point $(x_i, t_n)$. Pour le schéma
\begin{align} \label{eq:sch_exp}
\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n}-u_{i-1}^{n}}{2\Delta x} -b \frac{u_{i+1}^{n}+u_{i-1}^{n} - 2 u_i^{n}}{\Delta x^2} =0 \text{, on a}
\end{align}
\begin{multline*}
u(x_{i \pm 1}, t_n)= u(x_{i}, t_{n}) \pm \Delta x \frac{\partial u}{\partial x} (x_{i}, t_{n}) + \frac{1}{2} \Delta x^2 \frac{\partial^2 u}{\partial x^2}(x_{i}, t_{n}) \pm \frac{1}{6} \Delta x^3 \frac{\partial^3 u}{\partial x^3}(x_{i}, t_{n}) \\ + \frac{1}{24} \Delta x^4 \frac{\partial^4 u}{\partial^4 x} (x_{i}, t_{n}) + \O (\Delta x^5) \text{,}
\end{multline*}
\begin{align*}
u(x_{i}, t_{n+1})=u(x_{i}, t_{n}) + \Delta t \frac{\partial u}{\partial t} (x_{i}, t_{n}) + \frac{1}{2} \Delta t^2 \frac{\partial^2 u}{\partial t^2} (x_{i}, t_{n}) + \O (\Delta t^3) \text{.}
\end{align*}

On remplace le développement dans l'erreur de troncature qui est définit comme
\begin{align*}
\begin{split}
E&=\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n}-u_{i-1}^{n}}{2\Delta x} + b \frac{u_{i+1}^{n}+u_{i-1}^{n} - 2 u_i^{n}}{\Delta x^2} \\
&= \frac{\partial u}{\partial t}(x_i, t_n) + a \frac{\partial u}{\partial x}(x_i, t_n) - b \frac{\partial^2 u}{\partial^2 t}(x_i, t_n) + \O (\Delta t + \Delta x^2)
\end{split}
\end{align*}

Le erreur ci-dessus est dans le "standard" mentionné dans le Remarque 2.2.5 du polycopié, c'est à dire, pour une fonction régulière qui n'est pas solution de notre problème, la limite de l'erreur de troncature n'est pas nulle. Alors c'est schéma est consistante et d'ordre 1 en temps et 2 en espace.
Pour le schéma
\begin{align} \label{eq:sch_imp}
\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1}}{2\Delta x} -b \frac{u_{i+1}^{n+1} + u_{i-1}^{n+1} - 2 u_i^{n+1}}{\Delta x^2} =0 \text{,}
\end{align}
on fait le développement au tour du point $(x_i, t_{n+1})$ et on trouve
\begin{multline*}
u(x_{i \pm 1}, t_{n+1})= u(x_{i}, t_{n+1}) \pm \Delta x \frac{\partial u}{\partial x} (x_{i}, t_{n+1}) + \frac{1}{2} \Delta x^2 \frac{\partial^2 u}{\partial x^2}(x_{i}, t_{n+1}) \pm \frac{1}{6} \Delta x^3 \frac{\partial^3 u}{\partial x^3}(x_{i}, t_{n+1}) \\ + \frac{1}{24} \Delta x^4 \frac{\partial^4 u}{\partial^4 x} (x_{i}, t_{n+1}) + \O (\Delta x^5) \text{,}
\end{multline*}
\begin{align*}
u(x_{i}, t_{n})=u(x_{i}, t_{n+1}) - \Delta t \frac{\partial u}{\partial t} (x_{i}, t_{n+1}) + \frac{1}{2} \Delta t^2 \frac{\partial^2 u}{\partial t^2} (x_{i}, t_{n+1}) + \O (\Delta t^3) \text{, et finalement}
\end{align*}

\begin{align*}
\begin{split}
E&=\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1}}{2\Delta x} -b \frac{u_{i+1}^{n+1}u_{i-1}^{n+1} - 2 u_i^{n+1}}{\Delta x^2} \\
&= \frac{\partial u}{\partial t}(x_i, t_{n+1}) + a \frac{\partial u}{\partial x}(x_i, t_{n+1}) - b \frac{\partial^2 u}{\partial^2 t}(x_i, t_{n+1}) + \O (\Delta t + \Delta x^2) \text{.}
\end{split}
\end{align*}

Par le même argument du schéma antérieur, ce schéma est consistant d'ordre 1 en temps et 2 en espace.

\subsubsection
On commence par le schéma explicite \ref{eq:sch_exp}. Pour la stabilité $l^\infty$, on va trouver les conditions CFL pour que le schéma vérifie le principe du maximum discret.

Soit $k$ et $l$ tel que
\begin{align*}
u_k^{n+1}=M=\underset{j}{\max}\,u_j^{n+1} \quad \text{et} \quad u_l^{n+1}=M=\underset{j}{\min}\,u_j^{n+1}
\end{align*}

Notons que $M$ est positif ou nul et $m$ est négatif ou nul. On veut trouver les conditions pour que
\begin{align} \label{eq:M}
M \leq \max (0, \underset{j}{\max} \, u_j^n)
\end{align}
\begin{align} \label{eq:m}
\text{et} \min (0, \underset{j}{\min} \, u_j^n ) \leq m \text{.}
\end{align}

On commence l'analyse par l'inégalité \ref{eq:M}. Elle est trivialement vérifiée si $M=0$. Alors, on analyse le cas où $M\neq 0$. Le maximum de $u_j^{n+1}$ pour tout $j \in \{ i_{\text{max}}-1, \, i_{\text{max}}+1 \}$ est atteint en un élément $k \in \{ i_{\text{max}}, \, i_{\text{max}} \}$. Alors
\begin{align*}
\frac{M-u_k^{n}}{\Delta t} + a \frac{u_{k+1}^{n}-u_{k-1}^{n}}{2\Delta x} -b \frac{u_{k+1}^{n}+u_{k-1}^{n} - 2 u_k^{n}}{\Delta x^2} \leq 0
\end{align*}
soit
\begin{align*}
M \leq \left ( -\frac{a \Delta t}{2 \Delta x} + \frac{b \Delta t}{\Delta x^2} \right ) u_{k+1}^{n} + \left ( 1 -  \frac{2 b \Delta t}{\Delta x^2} \right ) u_{k}^{n} + \left ( \frac{a \Delta t}{2 \Delta x} + \frac{b \Delta t}{\Delta x^2} \right ) u_{k-1}^{n}
\end{align*}

Pour que le schéma de droite soit une combinaison convexe de $u_n$ et l'inégalité \ref{eq:M} soit vérifiée on les conditions ci-dessus.
\begin{align*}
\Delta x \leq \frac{2b}{a} \quad \text{,} \quad \Delta x \geq - \frac{2b}{a} \quad \text{et} \quad \Delta t \leq \frac{\Delta x^2}{2b} \text{.}
\end{align*}

Pour vérifiée l'inégalité \ref{eq:m}, on  remplace $u^n$ par $-u^n$ et $M$ par $-m$.

Pour la stabilité du schéma \ref{eq:sch_imp} en $l^\infty$, on voit que c'est un schéma implicite qui vérifie le principe du maximum. Alors, il est stable.

Pour la stabilité du schéma \ref{eq:sch_exp} en $l^2$, on a, pour la transformé de Fourier,
\begin{align*}
\hat{u}^{n+1}(k) = \left ( 1- \frac{a \Delta t}{2 \Delta x} (e^{2i\pi k \Delta x} - e^{-2i\pi k \Delta x}) + \frac{b \Delta t}{\Delta x^2}(e^{2i\pi k \Delta x} + e^{-2i\pi k \Delta x} -2) \right ) \hat{u}^{n}(k)
\end{align*}

Autrement dit 

\begin{align*}
\hat{u}^{n+1}(k) = A(k) \hat{u}^n(k)= A(k)^{n+1}\hat{u}^0(k)\text{ avec }A(k) = \left ( 1 - \frac{i a \Delta t}{2 \Delta x} \text{sin}(2 \pi k \Delta x) + \frac{4 b \Delta t}{\Delta x^2}\text{sin}^2(\pi k \Delta x) \right )
\end{align*}

Comme $\| A(k) \| \leq 1$ pour tout mode de Fourier k, la formule de Plancherel permet de conclure à la stabilité $l^2$ du schéma. (Voir polycopié page 36). 

\subsection{Schéma implicite}
\subsubsection{}
On peut récrire le schéma implicite comme
\begin{align*}
u_i^n= \left ( \frac{a \Delta t}{2 \Delta x} - \frac{b \Delta t}{\Delta x^2} \right ) u_{i+1}^{n+1} + \left ( 1 +  \frac{2 b \Delta t}{\Delta x^2} \right ) u_{i}^{n+1} + \left ( - \frac{a \Delta t}{2 \Delta x} - \frac{b \Delta t}{\Delta x^2} \right ) u_{i-1}^{n+1} \text{.}
\end{align*}
On pose $\gamma = \frac{a \Delta t}{2 \Delta x}$ et $\delta=\frac{b \Delta t}{\Delta x^2}$.
\begin{comment}
\begin{align*}
\gamma = \frac{a \Delta t}{2 \Delta x} \qquad \textrm{et} \qquad \delta=\frac{b \Delta t}{\Delta x^2} \text{.}
\end{align*}
\end{comment}
Alors pour $U^n=MU^{n+1}$, où $U^n= \begin{pmatrix} u_{-i_\text{max}}^n \\ \vdots \\ u_{i_\text{max}}^n \end{pmatrix}$, on trouve
\begin{comment}
\begin{align*}
M=\begin{pmatrix} 
 1 + 2 \delta & \gamma - \delta & 0 & \dots  & 0 \\
 -\gamma - \delta & 1 + 2 \delta & \gamma - \delta & \dots  & 0 \\
 \vdots & -\gamma - \delta & \ddots & \ddots & \vdots \\
  0 & 0 & \ddots & 1 + 2 \delta  & \gamma - \delta \\
 0 & 0 & \dots & -\gamma - \delta &  1 + 2 \delta
\end{pmatrix}
\end{align*}
$
M_{ij}=
\begin{cases}
1 + 2 \delta &\text{, si } i=j \\
\gamma - \delta  &\text{, si } i=j-1 \\
- (\gamma + \delta) &\text{, si } i=j+1 \\
0 &\text{, sinon}
\end{cases}
$.
\end{comment}

\begin{align*}
M = \begin{pmatrix}
1 + 2 \delta & \gamma - \delta\\
- (\gamma + \delta) & 1 + 2 \delta & \gamma - \delta \\
& - (\gamma + \delta)& \ddots & \ddots \\
& & \ddots & \ddots & \gamma - \delta \\
& & & - (\gamma + \delta) & 1 + 2 \delta
\end{pmatrix}
\end{align*}
\subsubsection{}
On définit $M^\mathcal{S} =\frac{1}{2} (M + M^T) $
\begin{comment}
M^s =\frac{1}{2} (M + M^T) = M^s_{ij}=
\begin{cases}
1 + 2 \delta &\text{, si } i=j \\
- \delta &\text{, si } i =  j \pm 1 \\
0 &\text{, sinon}
\end{cases}
$.
\end{comment}
\begin{align}
M^\mathcal{S} = \begin{pmatrix}
1 + 2 \delta & - \delta\\
- \delta & 1 + 2 \delta & - \delta \\
& - \delta & \ddots & \ddots \\
& & \ddots & \ddots & - \delta \\
& & & - \delta & 1 + 2 \delta
\end{pmatrix}
\end{align}

D'après \cite{weiss}, M est positive définit si et seulement si $M^\mathcal{S}$ y est. D'après \cite{milica}, comme ${\| 1 +2 \delta \| > \| - \delta \| + \| - \delta \|}$, $M^\mathcal{S}$ est définit positive, alors $M$ est définit positive. Ce qu'implique que $M$ est inversible.

\subsubsection{}
La solution a été calculé e la figure crée par le code disponible sur "problem5.py".
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{III23.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Représentation de la solution implicite pour $\Delta x = 0.1$, $\Delta t=10^{-4}$ et $n = 10^{5}$. Les fonctions sont superposées et l'intensité de la couleur rouge augmente avec le temps.}
	\label{fig:III23}
\end{figure}
\section{Application à l'équation de la chaleur non linéaire}
\subsection{Schéma implicite linéaire}

\subsubsection{}
On peut récrire le schéma comme
\begin{align*}
u_i^n = \left ( -\frac{b \Delta t}{\Delta x^2} \right) u_{i+1}^{n+1} +\left ( 1 + 2 \frac{b \Delta t}{\Delta x^2} \right )  u_{i}^{n+1}  + \left ( -\frac{b \Delta t}{\Delta x^2} \right) u_{i-1}^{n+1} + u_i^{n+1}(u_i^n)^3 \Delta t \text{.}
\end{align*}
Ainsi, on peut écrire $U^n=(A+ \Delta t \;\text{diag}((u_{-i_{\text{max}}}^n)^3, \dots, (u_{i_{\text{max}}}^n)^3))U^{n+1}$
, où $U^n= \begin{pmatrix} u_{-i_\text{max}}^n \\ \vdots \\ u_{i_\text{max}}^n \end{pmatrix}$, $\delta=\frac{b \Delta t}{\Delta x^2}$ et$
A = \begin{pmatrix}
1 + 2 \delta & - \delta\\
- \delta & 1 + 2 \delta & - \delta \\
& - \delta & \ddots & \ddots \\
& & \ddots & \ddots & - \delta \\
& & & - \delta & 1 + 2 \delta 
\end{pmatrix}$.

\begin{comment}
$A_{ij}= \begin{cases}
1 + 2 \delta &\text{, si } i=j \\
- \delta  &\text{, si } i=j \pm1 \\
0 &\text{, sinon}
\end{cases}$.
\end{comment}

\newpage
\subsubsection{}
La solution a été calculé e la figure crée par le code disponible sur "problem5.py".
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{IV12.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Représentation de la solution implicite pour $\Delta x = 0.1$, $\Delta t=10^{-4}$ et $n = 10^{5}$. Les fonctions sont superposées et l'intensité de la couleur rouge augmente avec le temps.}
	\label{fig:IV12}
\end{figure}

\subsection{Schéma pour l'équation stationnaire}
	
\subsubsection{}
On va écrire la fonction $G$ comme
\begin{align*}
G_i(U) = - b \frac{u_{i+1}+u_{i-1}-2u_{i}}{\Delta x^2} + u_i^4 - Q_i \text{, } \forall i \in [-i_{\text{max}}, i_{\text{max}}]
\end{align*}
de façon que pour $G=(G_{-i_{\text{max}}}, \dots ,G_{i_{\text{max}}})$, on a $G(U)=0$.

\subsubsection{}
On sait que $\left[DG(\beta)\right]_{ij} = \frac{\partial G_i(\beta)}{\partial \beta_j}$. Pour Q=1 et $\theta = \frac{b}{\Delta x^2}$, on a
\begin{align*}
DG(U) = \begin{pmatrix}
2\theta + 4 u_{-i_{\text{max}}}^3  & - \theta\\
- \theta & 2\theta + 4 u_{-i_{\text{max}}+1}^3 & - \theta \\
& - \theta & \ddots & \ddots \\
& & \ddots & \ddots & - \theta \\
& & & -\theta & 2\theta +4 u_{i_{\text{max}}}^3 
\end{pmatrix} \text{.}
\end{align*}
Pour l'algorithme de Newton on aura
\begin{align*}
U^{k_1}= U^k - (DG(U^k))^{-1}G(U^k), \text{ou alors}
\end{align*}
\begin{align*}
G(U^k)=DG(U^k)(U^k-U^{k+1}) \text{.}
\end{align*}
Le choix de la fonction initiale est arbitraire et on peut utiliser par exemple une parabole $u_0(x) = 1-\frac{x^2}{100}$. On voit la convergence vers une fonction proche que 1 au milieu et avec une courbature qui compense les conditions de Dirichlet aux bords.
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{IV22.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Représentation de la solution du problème stationnaire avec $\Delta x = 0.01$ et $n=20$ itérations. Les fonctions sont superposées et l'intensité de la couleur rouge augmente avec le temps.} 
	\label{fig:IV22}
\end{figure}
Le calcule et la figure ont été faits par le code "problem6.py".
\begin{thebibliography}{3}

	\bibitem{weiss} 
	Weisstein, Eric W.
	\textit{Positive Definite Matrices}. 
	From MathWorld--A Wolfram Web Resource. \url{http://mathworld.wolfram.com/PositiveDefiniteMatrix.html}
	
    \bibitem{milica} 
	Milica Anđelić and C. M. da Fonseca.
	\textit{Sufficient Conditions for Positive Definiteness of Tridiagonal Matricies Revisited}.
   \url{http://www.mat.uc.pt/~cmf/papers/tri_positive_definite_revisited.pdf}
    
\end{thebibliography}
\end{document}