%http://www.cmap.polytechnique.fr/~allaire/map411/MiniProjets/sujets2016/sujet_Augier.pdf

% --------------------------------------------------------------
%                         Template
% --------------------------------------------------------------

\documentclass[12pt]{article} %draft = show box warnings
\usepackage[utf8]{inputenc} % Accept different input encodings [utf8]
\usepackage[T1]{fontenc}    % Standard package for selecting font encodings
\usepackage{lmodern} % Good looking T1 font
\usepackage[a4paper, total={6.5in,10.2in}]{geometry} % Flexible and complete interface to document dimensions
\renewcommand{\baselinestretch}{1.0}

% --------------------------------------------------------------
%                         Other Fonts
% --------------------------------------------------------------
%\usepackage{mathpazo} % Hermann Zapf's Palatino font
%\usepackage{kpfonts} % Kepler font
%\usepackage{mathptmx} % Times New Roman Like Font
%\usepackage{eulervm} %  AMS Euler (eulervm) math font.

% --------------------------------------------------------------
%                         Package
% --------------------------------------------------------------
\usepackage[french]{babel} % Multilingual support [french]
\usepackage{enumerate} % Enumerate with redefinable labels
\usepackage{graphicx} % Enhanced support for graphics
\usepackage[makeroom]{cancel} % Place lines through maths formulae
\usepackage{booktabs} % Publication quality tables
\usepackage{braket} % Dirac bra-ket and set notations
\usepackage{epstopdf} % Convert EPS to ‘encapsulated’ PDF using Ghostscript
\usepackage{bbm} % "Blackboard-style" cm fonts
\RequirePackage{epsfig} % Include Encapsulated PostScript
\usepackage{float} % Improved interface for floating objects
\usepackage{amsmath,amsthm,amssymb} % American Mathematics Society facilities

\usepackage{stmaryrd} % St Mary Road symbols for theoretical computer science.
\usepackage{xfrac} % Create inline fraction
\usepackage{hyperref} % Create hyperf
\usepackage{secdot} % Put dots at the end of a section
\usepackage{comment}



%------------------- tikz --------------------------------------
\usepackage{tikz,bm,color}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{decorations}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\usetikzlibrary{shapes.geometric,arrows,positioning}
\usetikzlibrary{calc}

% --------------------------------------------------------------
%                         Custom commands
% --------------------------------------------------------------
\newcommand*{\1}{\mathbbm{1}}
\newcommand*{\E}{\mathbb{E}}
\newcommand*{\Pb}{\mathbb{P}}
\newcommand*{\N}{\mathbb{N}} 
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\car}{\text{card}}
\renewcommand*{\Re}{\operatorname{Re}}
\renewcommand*{\Im}{\operatorname{Im}}
\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}
\newcommand*{\grad}{\nabla}
\renewcommand*{\O}{\mathcal{O}}

% --------------------------------------------------------------
%                         Exercise Environment
% --------------------------------------------------------------
\renewcommand{\thesection}{\Roman{section}}
\sectiondot{subsection}
\renewcommand{\subsubsection}{
	\pagebreak[2]
	\refstepcounter{subsubsection}
	\noindent
	\textbf{\thesubsubsection.}
}
% --------------------------------------------------------------

\begin{document}
	% --------------------------------------------------------------
	%                       Title  Header
	% --------------------------------------------------------------
	\noindent
	\normalsize\textbf{Approximation numérique et optimisation} \hfill \textbf{École Polytechnique}\\
	\normalsize\textbf{MAP 411} \hfill \textbf{\today}\vspace{20pt}
	\centerline{\Large Projet MAP411 – X2015}\vspace{5pt}
	\centerline{\Large \textbf{Applications de méthodes d'optimisation à la résolution des EDP}}\vspace{3pt}
	\centerline{Sujet proposé par Nicolas Augier -- \texttt{nicolas.augier@ens-cachan.fr}  }\vspace{13pt}
	\centerline{Lucas Lugão Guimarães -- \texttt{lucas.lugao-guimaraes@polytechnique.edu}  }
	\centerline{Alexandre Ribeiro João Macedo --  \texttt{alexandre.macedo@polytechnique.edu}}\vspace{20pt}
	% --------------------------------------------------------------
	
	\section{Méthodes de gradient}
	\subsection{Gradient à pas constant}
	\subsubsection{} %%I.1.1
	La figure \ref{fig:I11} montre les lignes de niveau de la fonction de Rosenbrock,
	\begin{align} \label{eq:rosenbrock}
	J(u_1,u_2)=(u_1-1)^2+100(u_1^2-u_2)^2 \text{,}
	\end{align}
	dans le rectangle $(u_1, u_2) \in [-1.5, 1.5] \textmd{ x } [0.5, 1.5]$.   
	\begin{figure}[ht]
		\begin{center}
    \hspace*{-1cm}  
	\includegraphics[height=8cm]{I11.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Lignes de niveau de $J$.}
		\label{fig:I11}
	\end{figure}
	
    \begin{figure}[ht]
		\begin{center}
    \hspace*{-2cm}  
	\includegraphics[height=8cm]{I13d.png}
        \vspace{-1cm}
		\end{center}
		\caption{Surface plot de la fonction de Rosenbrock.}
		\label{fig:I13d}
	\end{figure}
	\subsubsection{} %% I.1.2
	On applique la méthode de gradient a pas constant $u_{k+1}=u_k-\rho \grad J(u_k)$ ou, pour $ u_k= (u_{k1},u_{k2})$, on a $\grad J(u_k) = ({400u_{k1}^3 + 2u_{k1}  - 400u_{k1}u_{k2} - 2}, {200u_{k1}^2 - 200u_{k2}})$. Pour les valeurs numériques $u_0=(1, 0)$ et $\rho = 0.002$, et le critère d'arrêt $J(u_{k+1}) < 10^{-3}$, on a besoin de \textbf{$2764$} itérations. La figure \ref{fig:I12p} montre les lignes de niveau de l'équation \ref{eq:rosenbrock} et le parcours du méthode avant l'arrête.
	\begin{figure}[H]
		\begin{center}
	\includegraphics[height=8cm]{I12002.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Parcours du méthode de gradient à pas constant.}
		\label{fig:I12p}
	\end{figure}
	
	\subsubsection{} %% I.1.3
	Si on applique la méthode pour $\rho = 0.0045$ et pour $\rho = 0.01$, on a que le méthode diverge. Pour la valeur $\rho = 0.0045$, la méthode oscille en tour du minimum et ne converge pas. Pour la valeur  $\rho = 0.01$, la première itération est déjà très loin du point de minimum et il ne reviens pas.
	\begin{figure}[H]
		\begin{center}
	\includegraphics[height=5cm]{I12d.eps}
        \vspace{-1cm}
		\end{center}
		\caption{Le parcours du méthode de gradient à pas constant $\rho = 0.045$ et $\rho = 0.01$.}
		\label{fig:I12d}
    \end{figure}
    \subsection{Gradient optimal}
\subsubsection{} %% II.1.1
	Soit $J(u)=\frac{1}{2} \langle Au, u \rangle - \langle b, u \rangle$. On veut trouver ${\rho_k=\textrm{argmin}_{\rho \geq 0} J(u_k- \rho \grad J(u_k))}$. Soit ${r_k= \grad J(u_k) = Au_k-b}$, on a
	\begin{align*}
	J(u_k - \rho \grad J(u_k)) &= J(u_k - \rho r_k) \\ 
	&=  \frac{1}{2} \langle A(u_k - \rho r_k), u_k - \rho r_k \rangle - \langle b, u_k - \rho r_k \rangle \\
	&=\frac{1}{2} \langle Au_k, u_k \rangle - \langle b, u_k \rangle  + \frac{1}{2} \langle Au_k, - \rho r_k \rangle + \frac{1}{2} \langle - \rho A r_k, u_k \rangle + \frac{1}{2} \langle - \rho A r_k,  - \rho r_k \rangle  \\
	& \hphantom{==\frac{1}{2} \langle Au_k, u_k \rangle - \langle b, u_k \rangle  + \frac{1}{2} \langle Au_k, - \rho r_k \rangle + \frac{1}{2} \langle - \rho A r_k, u_k \rangle } - \langle b, - \rho r_k \rangle \\
    &= J(u_k) - \frac{\rho}{2} [\langle A u_k, r_k \rangle + \langle A r_k, u_k \rangle]+\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho \langle b,  r_k \rangle \\
    &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho [\langle b,  r_k \rangle -\langle A u_k, r_k \rangle] \displaybreak[4]\\
    &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho [\langle b,  r_k \rangle -\langle r_k - b, r_k \rangle] \\
      &=J(u_k)  +\frac{\rho^2}{2} \langle A r_k, r_k \rangle + \rho \langle r_k, r_k \rangle \textrm{.}
	\end{align*}
    
Alors, la valeur que minimise cette fonction quadratique en $\rho$ est 
	\begin{align*}
	\rho_k=\frac{\langle r_k, r_k \rangle}{\langle A r_k, r_k \rangle}
	\end{align*}
\subsubsection
Pour les valeur $A=\begin{pmatrix} 1 & 0 \\  0 & \lambda \end{pmatrix}$ et  $b= \begin{pmatrix} 1 \\1 \end{pmatrix}$ on a
\begin{align*}
    	\begin{pmatrix}
    	r_{k1} \\
    	r_{k2}
    	\end{pmatrix}
    =    
 \begin{pmatrix}
1 & 0 \\
0 & \lambda
\end{pmatrix} 
\begin{pmatrix}
u_{k1} \\
u_{k2}
\end{pmatrix}
-
    \begin{pmatrix}
1 \\
1
\end{pmatrix}
=
 \begin{pmatrix}
u_{k1} - 1 \\
\lambda u_{k2} -1
\end{pmatrix}
\text{. Alors,}
\end{align*}
\begin{align} \label{eq:rho_k}
\rho_k = \frac{(u_{k1} - 1)^2 + (\lambda u_{k2} - 1)^2}{(u_{k1} - 1)^2 + \lambda (\lambda u_{k2} - 1)^2 }
\end{align}

À chaque itération, on actualise $\rho_k$ en utilisant l'équation \ref{eq:rho_k}. Le figure \ref{fig:I23a} montre les résultats pour différentes condition initiales et pour le critère de arrête 
\begin{align*}
\frac{\| r_k \|}{\| r_0 \|} \leq 10^{-2}\text{.}
\end{align*}
\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I123a.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours du méthode de gradient à pas optimal.}
	\label{fig:I23a}
\end{figure}
\subsubsection
Les figure \ref{fig:I23a} et \ref{fig:I23b} montre en échelle semi logarithme le rapport $\frac{\| r_k \|}{\| r_0 \|}$ en fonction de $k$ pour les cas montrés dans les figure \ref{fig:I23a} et \ref{fig:I23b}.

\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I123b.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours du méthode de gradient à pas optimal.}
	\label{fig:I23b}
\end{figure}
	
\subsubsection
Quand on met en œuvre le procédé de dichotomie pour l'équation \ref{eq:rosenbrock}, on trouve le résultat ci-dessous.
\begin{figure}[H]
	\begin{center}
\includegraphics[height=8cm]{I124.eps}
    \vspace{-1cm}
	\end{center}
	\caption{Parcours obtenu à partir du procédé de dichotomie pour trouver le minimum global de la fonction de Rosenbrock.}
	\label{fig:I24}
\end{figure}
	
\section{Méthodes newtoniennes}
\subsection{Méthode de Newton}
\subsection{Méthode de Gauss-Newton}
\subsubsection{}
On note le vecteur des paramètres $\beta = \{\beta_1, \dots, \beta_{3K}\}$ de manière que $\beta_j = \alpha_j$, $\beta_{j+K} = \sigma_{j}$ et $\beta_{j+2K} = x^0_j$. Ainsi la matrice jacobienne $Df(\beta)$ de dimension $N\times3K$ est donné par $\left[Df(\beta)\right]_{ij} = \frac{\partial f_i(\beta)}{\partial \beta_j}$. À fin de la calculer on rappelle la définition du vecteur $f$ $f_i(\beta) = y_i - g(x_i, \beta)$ avec $g(x_i,\beta) = \sum_{j = 1}^K \alpha_j \exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right)$
\begin{align*}
\left[Df(\beta)\right]_{ij} = \frac{\partial f_i(\beta)}{\partial \alpha_j} &= - \exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K\\
\left[Df(\beta)\right]_{i(K+j)} = \frac{\partial f_i(\beta)}{\partial \sigma_j} &= -\alpha_j \frac{(x-x_j^0)^2}{\sigma_j^3}\exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K\\
\left[Df(\beta)\right]_{i(2K+j)} =\frac{\partial f_i(\beta)}{\partial x_j^0} &= -\alpha_j \frac{(x-x_j^0)}{\sigma_j^2}\exp\left(-\frac{1}{2} \frac{(x-x_j^0)^2}{\sigma_j^2}\right),\quad 1\leq j \leq K
\end{align*}

On écrit le processus itératif à partir de l'algorithme de Newton $\beta_{k+1} = \beta_{k} - [DF(\beta_k)]^{-1}F(\beta_k)$ en prenant l'approximation $DF(\beta_k) = D [\nabla J(\beta_k)] = \nabla^2 J(\beta_k) \approx Df^T(u)Df(u)$ et en rappelant que $F(\beta_k) = \nabla J(\beta_k) =  Df^T(\beta_{k})f(\beta_{k})$
\begin{align*}
\beta_{k+1} = \beta_{k} - [Df^T(\beta_{k})Df(\beta_{k})]^{-1} Df^T(\beta_{k})f(\beta_{k}),\quad k\geq 0
\end{align*}
\subsubsection{}
Avec les conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 7$ la méthode de Gauss-Newton converge vers les valeurs suivantes
\begin{align*}
\alpha_1 = 0.22213775,\quad
\alpha_2 = 0.98932187,\quad
\sigma_1 = 1.27128674,\\
\sigma_2 = 2.64012043,\quad
x^0_1 = 2.57935799,\quad
x^0_2 = 9.16230523          
\end{align*}
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{I22.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Nuage de points obtenue à partir des conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 7$}
	\label{fig:I22}
\end{figure}
\subsubsection{}
D'après la figure \ref{fig:I23} on voit la divergence de la méthode de Gauss-Newton avec des conditions initiales proches de la solution trouvé dans l'item précédent. De cette manière on voit la non-convergence locale de cette approximation de la Méthode de Newton classique.
\begin{figure}[H]
	\begin{center}
		\includegraphics[height=8cm]{I23.eps}
		\vspace{-1cm}
	\end{center}
	\caption{Nuage de points obtenue à partir des conditions initiales $\alpha_1 = \alpha_2 = \sigma_1 = \sigma_2 = 1$, $x_1^0 = 3$ et $x_2^0 = 6$}
	\label{fig:I23}
\end{figure}
\section{Application à l'équation de convection-diffusion}
\subsection{Différences schémas d'approximation}
\subsubsection{}
On remplace $u_i^n$ par $u(x_i, t_n)$ où $u$ est une fonction régulière. On commence par le développement de Taylor au tour du point $(x_i, t_n)$. Pour le schéma
\begin{align} \label{eq:sch_exp}
\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n}-u_{i-1}^{n}}{2\Delta x} -b \frac{u_{i+1}^{n}-u_{i-1}^{n} - 2 u_i^{n}}{\Delta x^2} =0 \text{, on a}
\end{align}
\begin{multline*}
u(x_{i \pm 1}, t_n)= u(x_{i}, t_{n}) \pm \Delta x \frac{\partial u}{\partial x} (x_{i}, t_{n}) + \frac{1}{2} \Delta x^2 \frac{\partial^2 u}{\partial x^2}(x_{i}, t_{n}) \pm \frac{1}{6} \Delta x^3 \frac{\partial^3 u}{\partial x^3}(x_{i}, t_{n}) \\ + \frac{1}{24} \Delta x^4 \frac{\partial^4 u}{\partial^4 x} (x_{i}, t_{n}) + \O (\Delta x^5) \text{,}
\end{multline*}
\begin{align*}
u(x_{i}, t_{n+1})=u(x_{i}, t_{n}) + \Delta t \frac{\partial u}{\partial t} (x_{i}, t_{n}) + \frac{1}{2} \Delta t^2 \frac{\partial^2 u}{\partial t^2} (x_{i}, t_{n}) + \O (\Delta t^3) \text{.}
\end{align*}

On remplace le développement dans l'erreur de troncature qui est définit comme
\begin{align*}
\begin{split}
E&=\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n}-u_{i-1}^{n}}{2\Delta x} -b \frac{u_{i+1}^{n}-u_{i-1}^{n} - 2 u_i^{n}}{\Delta x^2} \\
&= \frac{\partial u}{\partial t}(x_i, t_n) + a \frac{\partial u}{\partial x}(x_i, t_n) - b \frac{\partial^2 u}{\partial^2 t}(x_i, t_n) + \O (\Delta t + \Delta x^2)
\end{split}
\end{align*}

Le erreur ci-dessus est dans le "standard" mentionné dans le Remarque 2.2.5 du polycopié, c'est à dire, pour une fonction régulière qui n'est pas solution de notre problème, la limite de l'erreur de troncature n'est pas nulle. Alors c'est schéma est consistante et d'ordre 1 en temps et 2 en espace.
Pour le schéma
\begin{align} \label{eq:sch_imp}
\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1}}{2\Delta x} -b \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1} - 2 u_i^{n+1}}{\Delta x^2} =0 \text{,}
\end{align}
on fait le développement au tour du point $(x_i, t_{n+1})$ et on trouve
\begin{multline*}
u(x_{i \pm 1}, t_{n+1})= u(x_{i}, t_{n+1}) \pm \Delta x \frac{\partial u}{\partial x} (x_{i}, t_{n+1}) + \frac{1}{2} \Delta x^2 \frac{\partial^2 u}{\partial x^2}(x_{i}, t_{n+1}) \pm \frac{1}{6} \Delta x^3 \frac{\partial^3 u}{\partial x^3}(x_{i}, t_{n+1}) \\ + \frac{1}{24} \Delta x^4 \frac{\partial^4 u}{\partial^4 x} (x_{i}, t_{n+1}) + \O (\Delta x^5) \text{,}
\end{multline*}
\begin{align*}
u(x_{i}, t_{n})=u(x_{i}, t_{n+1}) - \Delta t \frac{\partial u}{\partial t} (x_{i}, t_{n+1}) + \frac{1}{2} \Delta t^2 \frac{\partial^2 u}{\partial t^2} (x_{i}, t_{n+1}) + \O (\Delta t^3) \text{, et finalement}
\end{align*}

\begin{align*}
\begin{split}
E&=\frac{u_i^{n+1}-u_i^{n}}{\Delta t} + a \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1}}{2\Delta x} -b \frac{u_{i+1}^{n+1}-u_{i-1}^{n+1} - 2 u_i^{n+1}}{\Delta x^2} \\
&= \frac{\partial u}{\partial t}(x_i, t_{n+1}) + a \frac{\partial u}{\partial x}(x_i, t_{n+1}) - b \frac{\partial^2 u}{\partial^2 t}(x_i, t_{n+1}) + \O (\Delta t + \Delta x^2) \text{.}
\end{split}
\end{align*}

Par le même argument du schéma antérieur, ce schéma est consistant d'ordre 1 en temps et 2 en espace.

\subsubsection
content...

\subsection{Schéma implicite}
\subsubsection{}
On peut récrire le schéma implicite comme
\begin{align*}
u_i^n= \left ( \frac{a \Delta t}{2 \Delta x} - \frac{b \Delta t}{\Delta x^2} \right ) u_{i+1}^{n+1} + \left ( 1 +  \frac{2 b \Delta t}{\Delta x^2} \right ) u_{i}^{n+1} + \left ( - \frac{a \Delta t}{2 \Delta x} - \frac{b \Delta t}{\Delta x^2} \right ) u_{i-1}^{n+1} \text{.}
\end{align*}
On pose $\gamma = \frac{a \Delta t}{2 \Delta x}$ et $\delta=\frac{b \Delta t}{\Delta x^2}$.
\begin{comment}
\begin{align*}
\gamma = \frac{a \Delta t}{2 \Delta x} \qquad \textrm{et} \qquad \delta=\frac{b \Delta t}{\Delta x^2} \text{.}
\end{align*}
\end{comment}
Alors pour $U^n=MU^{n+1}$, où $U^n= \begin{pmatrix} u_{-i_\text{max}}^n \\ \vdots \\ u_{i_\text{max}}^n \end{pmatrix}$, on trouve
\begin{comment}
\begin{align*}
M=\begin{pmatrix} 
 1 + 2 \delta & \gamma - \delta & 0 & \dots  & 0 \\
 -\gamma - \delta & 1 + 2 \delta & \gamma - \delta & \dots  & 0 \\
 \vdots & -\gamma - \delta & \ddots & \ddots & \vdots \\
  0 & 0 & \ddots & 1 + 2 \delta  & \gamma - \delta \\
 0 & 0 & \dots & -\gamma - \delta &  1 + 2 \delta
\end{pmatrix}
\end{align*}
$
M_{ij}=
\begin{cases}
1 + 2 \delta &\text{, si } i=j \\
\gamma - \delta  &\text{, si } i=j-1 \\
- (\gamma + \delta) &\text{, si } i=j+1 \\
0 &\text{, sinon}
\end{cases}
$.
\end{comment}

\begin{align*}
M = \begin{pmatrix}
1 + 2 \delta & \gamma - \delta\\
- (\gamma + \delta) & 1 + 2 \delta & \gamma - \delta \\
& - (\gamma + \delta)& \ddots & \ddots \\
& & \ddots & \ddots & \gamma - \delta \\
& & & - (\gamma + \delta) & 1 + 2 \delta
\end{pmatrix}
\end{align*}
\subsubsection{}
On définit 
\begin{comment}
M^s =\frac{1}{2} (M + M^T) = M^s_{ij}=
\begin{cases}
1 + 2 \delta &\text{, si } i=j \\
- \delta &\text{, si } i =  j \pm 1 \\
0 &\text{, sinon}
\end{cases}
$.
\end{comment}
\begin{align}
M^\mathcal{S} = \begin{pmatrix}
1 + 2 \delta & - \delta\\
- \delta & 1 + 2 \delta & - \delta \\
& - \delta & \ddots & \ddots \\
& & \ddots & \ddots & - \delta \\
& & & - \delta & 1 + 2 \delta
\end{pmatrix}
\end{align}

D'après \cite{weiss}, M est positive définit si et seulement si $M^\mathcal{S}$ y est. D'après \cite{milica}, comme ${\| 1 +2 \delta \| > \| - \delta \| + \| - \delta \|}$, $M^\mathcal{S}$ est définit positive, alors $M$ est définit positive. Ce qu'implique que $M$ est inversible.

\subsubsection{}
content...
\section{Application à l'équation de la chaleur non linéaire}
\subsection{Schéma implicite linéaire}

\subsubsection{}
On peut récrire le schéma comme
\begin{align*}
u_i^n = \left ( -\frac{b \Delta t}{\Delta x^2} \right) u_{i+1}^{n+1} +\left ( 1 + 2 \frac{b \Delta t}{\Delta x^2} \right )  u_{i}^{n+1}  + \left ( -\frac{b \Delta t}{\Delta x^2} \right) u_{i-1}^{n+1} + u_i^{n+1}(u_i^n)^3 \Delta t \text{.}
\end{align*}
Ainsi, on peut écrire $U^n=(A+ \Delta t \;\text{diag}((u_{-i_{\text{max}}}^n)^3, \dots, (u_{i_{\text{max}}}^n)^3))U^{n+1}$
, où $U^n= \begin{pmatrix} u_{-i_\text{max}}^n \\ \vdots \\ u_{i_\text{max}}^n \end{pmatrix}$, $\delta=\frac{b \Delta t}{\Delta x^2}$ et
\begin{align*}
A = \begin{pmatrix}
1 + 2 \delta & - \delta\\
- \delta & 1 + 2 \delta & - \delta \\
& - \delta & \ddots & \ddots \\
& & \ddots & \ddots & - \delta \\
& & & - \delta & 1 + 2 \delta 
\end{pmatrix}
\end{align*}

\begin{comment}
$A_{ij}= \begin{cases}
1 + 2 \delta &\text{, si } i=j \\
- \delta  &\text{, si } i=j \pm1 \\
0 &\text{, sinon}
\end{cases}$.
\end{comment}

\subsubsection{}

content...

\subsection{Schéma pour l'équation stationnaire}
	
\subsubsection{}
content...

\subsubsection{}
content...

\begin{thebibliography}{3}

	\bibitem{weiss} 
	Weisstein, Eric W.
	\textit{Positive Definite Matrices}. 
	From MathWorld--A Wolfram Web Resource. \url{http://mathworld.wolfram.com/PositiveDefiniteMatrix.html}
	
    \bibitem{milica} 
	Milica Anđelić and C. M. da Fonseca.
	\textit{Sufficient Conditions for Positive Definiteness of Tridiagonal Matricies Revisited}.
   \url{http://www.mat.uc.pt/~cmf/papers/tri_positive_definite_revisited.pdf}
    
\end{thebibliography}
\end{document}